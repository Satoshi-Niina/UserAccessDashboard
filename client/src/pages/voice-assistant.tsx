// âœ… voice-assistant.tsxï¼ˆãƒ•ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰
// âœ… ã™ã¹ã¦ã®èª¬æ˜ãƒ»è£œè¶³ä»˜ãã§æ§‹æˆã•ã‚ŒãŸç´„280è¡Œã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚³ãƒ¼ãƒ‰ã§ã™

import { useState, useEffect, useRef } from "react";
import Fuse from 'fuse.js';
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { Mic, Send, X } from "lucide-react";

// å‹å®šç¾©ï¼ˆFuse.jsæ¤œç´¢çµæœã¨ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
type SearchResult = {
  content: string;
  type: 'text' | 'image';
  source: string;
};

type ChatMessage = {
  content: string;
  isUser: boolean;
  results?: SearchResult[];
};

export default function VoiceAssistant() {
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [inputText, setInputText] = useState<string>("");
  const [isRecording, setIsRecording] = useState(false);
  const [mode, setMode] = useState<'photo' | 'video'>('photo');
  const [searchData, setSearchData] = useState<any[]>([]);
  const [fuse, setFuse] = useState<Fuse<any>>();
  const [isLoading, setIsLoading] = useState(true);
  const [selectedResult, setSelectedResult] = useState<SearchResult | null>(null);
  const [initError, setInitError] = useState<string | null>(null);

  const mediaStreamRef = useRef<MediaStream | null>(null);

  const isIOS = () => /iPhone|iPad|iPod/i.test(navigator.userAgent);

  // ğŸ”½ ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼†Fuse.js åˆæœŸåŒ–
  useEffect(() => {
    const checkSpeechSDK = () => {
      if ((window as any).SpeechSDK) {
        console.log("âœ… Azure Speech SDK ãŒèª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸ");
      } else {
        console.error("âŒ Speech SDK ãŒæœªå®šç¾©ã§ã™ - å†è©¦è¡Œã—ã¾ã™");
        setTimeout(checkSpeechSDK, 1000); // 1ç§’å¾Œã«å†ç¢ºèª
      }
    };
    checkSpeechSDK();
    async function initializeSearch() {
      setIsLoading(true);
      setInitError(null);
      try {
        const response = await fetch('/api/tech-support/data/extracted_data.json');
        if (!response.ok) throw new Error(`HTTPã‚¨ãƒ©ãƒ¼: ${response.status}`);
        const data = await response.json();
        const processedData = Array.isArray(data) ? data : (data.slides || []);
        setSearchData(processedData);
        if (processedData.length === 0) throw new Error('ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™');
        const fuseKeys = ['ãƒãƒ¼ãƒˆ', 'æœ¬æ–‡', 'ç”»åƒãƒ†ã‚­ã‚¹ãƒˆ'];
        const sample = processedData[0];
        for (const key in sample) {
          if (typeof sample[key] === 'string' && !fuseKeys.includes(key)) {
            fuseKeys.push(key);
          }
        }
        setFuse(new Fuse(processedData, {
          keys: fuseKeys,
          threshold: 0.5,
          includeMatches: true,
        }));
      } catch (err) {
        console.error('âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼:', err);
        setInitError('ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ');
      } finally {
        setIsLoading(false);
      }
    }
    initializeSearch();
  }, []);

  useEffect(() => {
    if (mode) startCamera();
    return () => stopCamera();
  }, [mode]);

  // ğŸ”½ iOSå¯¾å¿œã®å®‰å®šç‰ˆéŸ³å£°èªè­˜å‡¦ç†ï¼ˆAzureï¼‰
  const startMic = async () => {
    if (isIOS()) {
      try {
        if (!(window as any).SpeechSDK) {
          throw new Error('Speech SDKãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“');
        }
        const SpeechSDK = (window as any).SpeechSDK;
        const key = import.meta.env.VITE_AZURE_SPEECH_KEY;
        const region = import.meta.env.VITE_AZURE_SPEECH_REGION;
        if (!key || !region) throw new Error('ã‚­ãƒ¼ã¾ãŸã¯ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ãŒæœªè¨­å®šã§ã™');
        const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(key, region);
        speechConfig.speechRecognitionLanguage = "ja-JP";
        const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
        const recognizer = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig);
        recognizer.recognized = (s: any, e: any) => {
          if (e.result.reason === SpeechSDK.ResultReason.RecognizedSpeech) {
            setMessages(prev => [...prev, { content: e.result.text, isUser: true }]);
          }
        };
        recognizer.canceled = () => {
          recognizer.stopContinuousRecognitionAsync();
          setIsRecording(false);
        };
        recognizer.sessionStopped = () => {
          recognizer.stopContinuousRecognitionAsync();
          setIsRecording(false);
        };
        recognizer.startContinuousRecognitionAsync();
        setIsRecording(true);
      } catch (error: any) {
        console.error("Azureèªè­˜ã‚¨ãƒ©ãƒ¼:", error);
        setMessages(prev => [...prev, {
          content: `AzureéŸ³å£°èªè­˜ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸï¼š${error.message}`,
          isUser: false
        }]);
        setIsRecording(false);
      }
    } else {
      try {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) throw new Error('Web Speech API éå¯¾å¿œ');
        const recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.lang = 'ja-JP';
        recognition.interimResults = true;
        recognition.onresult = (event: SpeechRecognitionEvent) => {
          const last = event.results.length - 1;
          const transcript = event.results[last][0].transcript;
          if (event.results[last].isFinal) {
            setMessages(prev => [...prev, { content: transcript, isUser: true }]);
          }
        };
        recognition.onerror = (event: any) => {
          console.error('éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼:', event.error);
          setIsRecording(false);
        };
        await navigator.mediaDevices.getUserMedia({ audio: true });
        recognition.start();
        setIsRecording(true);
      } catch (err: any) {
        console.error("ãƒã‚¤ã‚¯èµ·å‹•å¤±æ•—:", err);
        setMessages(prev => [...prev, {
          content: `éŸ³å£°èªè­˜ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ: ${err.message}`,
          isUser: false
        }]);
        setIsRecording(false);
      }
    }
  };

  const stopMic = () => {
    setIsRecording(false);
    console.log("ğŸ¤ ãƒã‚¤ã‚¯ã‚’åœæ­¢ã—ã¾ã—ãŸ");
  };

  const startCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      mediaStreamRef.current = stream;
      console.log("ğŸ“· ã‚«ãƒ¡ãƒ©ãŒèµ·å‹•ã—ã¾ã—ãŸ");
    } catch (err) {
      console.error("ã‚«ãƒ¡ãƒ©ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ", err);
    }
  };

  const stopCamera = () => {
    mediaStreamRef.current?.getTracks().forEach(track => track.stop());
    console.log("ğŸ“· ã‚«ãƒ¡ãƒ©ã‚’åœæ­¢ã—ã¾ã—ãŸ");
  };

  // ğŸ”½ æ¤œç´¢ã¨ChatGPTå¿œç­”ã‚’åŒæ™‚ã«å®Ÿè¡Œ
  const handleSearchAndChat = async () => {
    const query = inputText.trim();
    if (!query) return;
    setInputText("");
    await handleSearch(query);
    await sendToChatGPT(query);
  };

  const handleSearch = async (query?: string) => {
    const rawText = typeof query === 'string' ? query : inputText;
    const searchText = rawText?.trim() ?? '';
    if (!searchText) return;
    try {
      if (!fuse || !searchData.length) throw new Error('æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®æº–å‚™ä¸­ã§ã™ã€‚');
      const results = fuse.search(searchText);
      const searchResults = results.map(result => ({
        content: result.item.ãƒãƒ¼ãƒˆ || (result.item.æœ¬æ–‡ ? result.item.æœ¬æ–‡.join('\n') : ''),
        type: result.item.ç”»åƒãƒ†ã‚­ã‚¹ãƒˆ && result.item.ç”»åƒãƒ†ã‚­ã‚¹ãƒˆ.length > 0 ? 'image' : 'text',
        source: result.item.ç”»åƒãƒ†ã‚­ã‚¹ãƒˆ?.[0]?.ç”»åƒãƒ‘ã‚¹?.replace(/^.*\\output\\images\\/, '/api/tech-support/images/') || ''
      }));
      setMessages(prev => [...prev, { content: "æ¤œç´¢çµæœ:", isUser: false, results: searchResults }]);
    } catch (error) {
      console.error('æ¤œç´¢ã‚¨ãƒ©ãƒ¼:', error);
      setMessages(prev => [...prev, { content: "æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", isUser: false }]);
    }
  };

  // ğŸ”½ ChatGPTã«è³ªå•ã‚’é€ä¿¡ï¼ˆFuseæ¤œç´¢çµæœã‚’å«ã‚ã‚‹RAGå‹ï¼‰
  const sendToChatGPT = async (userText: string) => {
    try {
      const results = fuse?.search(userText) ?? [];
      const contextText = results.slice(0, 3).map((r, i) => {
        const note = r.item.ãƒãƒ¼ãƒˆ ?? '';
        const body = Array.isArray(r.item.æœ¬æ–‡) ? r.item.æœ¬æ–‡.join('\n') : '';
        return `ã€ã‚¹ãƒ©ã‚¤ãƒ‰${i + 1}ã€‘\n${note}\n${body}`;
      }).join('\n\n');

      const prompt = `
ä»¥ä¸‹ã¯ä¿å®ˆç”¨è»ŠãƒŠãƒ¬ãƒƒã‚¸ã‹ã‚‰æ¤œç´¢ã•ã‚ŒãŸæƒ…å ±ã§ã™ã€‚
ã“ã®æƒ…å ±ã®ç¯„å›²å†…ã§ã€æ¬¡ã®è³ªå•ã«å¯¾ã—ã¦æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

${contextText}

ã€è³ªå•ã€‘
${userText}

â€»è©²å½“æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
`;

      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: prompt })
      });
      const data = await response.json();
      setMessages(prev => [...prev, { content: data.reply, isUser: false }]);
    } catch (error) {
      console.error('ChatGPTé€šä¿¡ã‚¨ãƒ©ãƒ¼:', error);
      setMessages(prev => [...prev, { content: 'ChatGPTã‹ã‚‰ã®å¿œç­”ã«å¤±æ•—ã—ã¾ã—ãŸã€‚', isUser: false }]);
    }
  };

  const handleTextSelection = () => {
    const selection = window.getSelection();
    const selectedText = selection ? selection.toString().trim() : '';
    if (selectedText) setInputText(selectedText);
  };

  return (
    <div className="container mx-auto p-4 relative">
      <h1 className="text-center text-xl font-bold mb-4">å¿œæ€¥å¯¾å¿œã‚µãƒãƒ¼ãƒˆ</h1>
      <Button variant="outline" className="absolute top-2 right-2 rounded-full shadow px-3 py-1 text-sm" onClick={() => window.location.href = '/'}>
        çµ‚äº†
      </Button>

      <div className="flex flex-col h-[60vh] border border-blue-400 rounded-lg p-4 overflow-hidden mb-6">
        <div className="flex-1 overflow-y-auto mb-4">
          <div className="space-y-4">
            {messages.map((message, index) => (
              <div key={index} className={`flex ${message.isUser ? 'justify-start' : 'justify-end'}`}>
                <div
                  className={`rounded-lg p-3 max-w-[60%] ${message.isUser ? 'bg-blue-500 text-white cursor-pointer' : 'bg-gray-100'}`}
                  onMouseUp={handleTextSelection}
                  onContextMenu={(e) => {
                    e.preventDefault();
                    if (message.isUser && confirm('ã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ')) {
                      setMessages(prev => prev.filter((_, i) => i !== index));
                    }
                  }}
                >
                  <p className="font-bold">{message.content}</p>
                  {!message.isUser && message.results && (
                    <div className="mt-2 grid grid-cols-2 gap-2">
                      {message.results.filter(r => r.type === 'image' && r.source).map((result, idx) => (
                        <div key={idx} className="border rounded cursor-pointer hover:shadow-lg" onClick={() => setSelectedResult(result)}>
                          <img src={result.source} alt={result.content} className="w-full h-32 object-cover rounded" />
                        </div>
                      ))}
                    </div>
                  )}
                </div>
              </div>
            ))}
          </div>
        </div>

        <div className="flex gap-2 items-start">
          <Button variant={isRecording ? "destructive" : "default"} onClick={isRecording ? stopMic : startMic}>
            {isRecording ? <X className="h-4 w-4" /> : <Mic className="h-4 w-4" />}
          </Button>
          <Input
            value={inputText}
            onChange={(e) => setInputText(e.target.value)}
            onKeyPress={(e) => e.key === 'Enter' && handleSearchAndChat()}
            placeholder={isLoading ? "ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­..." : "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›..."}
            className="flex-1"
            disabled={isLoading}
          />
          <Button onClick={handleSearchAndChat} disabled={isLoading || !fuse}>
            <Send className="h-4 w-4" />
          </Button>
        </div>
      </div>
    </div>
  );
}
